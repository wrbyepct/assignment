# tax_registration/management/commands/import_business.py
import logging
from pathlib import Path
import csv
import pandas as pd
import requests
from io import StringIO
from contextlib import contextmanager
from typing import Tuple, List
from datetime import datetime

from django.core.management.base import BaseCommand, CommandError
from django.db import transaction, connection
from django.utils import timezone
from requests.adapters import HTTPAdapter
from urllib3.util import Retry

from core.tax_registration.models import (
    TaxRegistration,
    BusinessIndustry,
    ETLJobRun,
    DataImportError,
    ImportProgress,
)

logger = logging.getLogger("tax_registration.etl")


class Command(BaseCommand):
    help = "åŒ¯å…¥å…¨åœ‹ç‡Ÿæ¥­ç™»è¨˜è³‡æ–™(å„ªåŒ–ç‰ˆ)"

    # é¡åˆ¥å¸¸æ•¸
    CSV_URL = "https://eip.fia.gov.tw/data/BGMOPEN1.csv"

    def __init__(self):
        super().__init__()
        self.stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "duplicates": 0,
            "skipped": 0,
        }
        self.job_run = None
        self.start_time = None

    def add_arguments(self, parser):
        parser.add_argument(
            "--batch-size",
            type=int,
            default=10000,
            help="æ¯æ‰¹æ¬¡è™•ç†ç­†æ•¸(å»ºè­° 5000-20000)",
        )
        parser.add_argument(
            "--chunk-size", type=int, default=50000, help="CSV è®€å– chunk å¤§å°"
        )
        parser.add_argument("--resume", action="store_true", help="å¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒ")
        parser.add_argument(
            "--dry-run", action="store_true", help="åªé©—è­‰è³‡æ–™ä¸å¯¦éš›åŒ¯å…¥"
        )
        parser.add_argument(
            "--truncate", action="store_true", help="æ¸…ç©ºç¾æœ‰è³‡æ–™å¾Œé‡æ–°åŒ¯å…¥(å±éšªæ“ä½œ!)"
        )
        parser.add_argument(
            "--limit", type=int, default=None, help="é™åˆ¶è™•ç†ç­†æ•¸(æ¸¬è©¦ç”¨)"
        )

    def handle(self, *args, **options):
        """ä¸»è¦é€²å…¥é»"""
        self.batch_size = options["batch_size"]
        self.chunk_size = options["chunk_size"]
        self.dry_run = options["dry_run"]
        self.resume = options["resume"]
        self.limit = options["limit"]

        # TODO check if this query is slow
        # TODO Explore other more effcient solutions
        # æª¢æŸ¥æ˜¯å¦æœ‰æ­£åœ¨åŸ·è¡Œä¸­çš„ä»»å‹™

        ongoing_job = ETLJobRun.objects.filter(status="running").exists()

        if ongoing_job:
            self.stdout.write(self.style.ERROR("å·²æœ‰ä»»å‹™æ­£åœ¨åŸ·è¡Œä¸­ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"))
            return  # çµ‚æ­¢åŸ·è¡Œ

        if options["truncate"] and options["resume"]:
            raise CommandError(
                "âŒ --truncate å’Œ --resume ä¸èƒ½åŒæ™‚ä½¿ç”¨\n"
                "   --truncate: æ¸…ç©ºè³‡æ–™å¾Œé‡æ–°åŒ¯å…¥\n"
                "   --resume: å¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒ"
            )

        # è¼‰å…¥æ–°è³‡æ–™å‰ç©º table
        if options["truncate"]:
            if not self._confirm_truncate():
                self.stdout.write(self.style.WARNING("æ“ä½œå·²å–æ¶ˆ"))
                return
            self._truncate_tables()

        # å»ºç«‹åŸ·è¡Œç´€éŒ„
        self.create_etl_job()

        try:
            self.handle_successful_etl_job()
        except Exception as e:
            self.handle_failed_etl_job(e)
        finally:
            self.job_run.completed_at = timezone.now()
            self.job_run.save()
            self._print_summary()

    def create_etl_job(self):
        """å‰µå»º ETL Job ä¸¦ log"""
        self.job_run = ETLJobRun.objects.create(
            status="running",
            batch_size=self.batch_size,
            chunk_size=self.chunk_size,
            data_source_url=self.CSV_URL,
        )
        logger.info(
            "ETL ä»»å‹™é–‹å§‹",
            extra={
                "event": "etl_started",
                "job_run_id": self.job_run.id,
                "batch_size": self.batch_size,
                "chunk_size": self.chunk_size,
                "dry_run": self.dry_run,
            },
        )

    def handle_successful_etl_job(self):
        """åŸ·è¡Œ ETL Job, æ›´æ–°æˆåŠŸçµæœ, log æˆåŠŸè¨Šæ¯"""
        with self._track_progress():
            self._run_etl()

        # æ›´æ–°åŸ·è¡Œçµæœ
        self.job_run.status = "success"
        self.job_run.records_total = self.stats["total"]
        self.job_run.records_processed = self.stats["success"]
        self.job_run.records_failed = self.stats["failed"]
        self.job_run.records_duplicated = self.stats["duplicates"]

        logger.info(
            "ETL ä»»å‹™å®Œæˆ",
            extra={
                "event": "etl_completed",
                "job_run_id": self.job_run.id,
                "status": "success",
                "records_total": self.stats["total"],
                "records_processed": self.stats["success"],
                "records_failed": self.stats["failed"],
            },
        )

    @contextmanager
    def _track_progress(self):
        """è¿½è¹¤åŸ·è¡Œé€²åº¦"""
        self.start_time = timezone.now()
        self.stdout.write(
            self.style.MIGRATE_HEADING(
                f"\n{'=' * 60}\né–‹å§‹åŸ·è¡Œ ETL (ID: {self.job_run.id})\n{'=' * 60}\n"
            )
        )
        yield  # _run_etl runs here
        duration = (timezone.now() - self.start_time).total_seconds()
        self.stdout.write(self.style.SUCCESS(f"\nåŸ·è¡Œæ™‚é–“: {duration:.2f} ç§’"))

    def handle_failed_etl_job(self, error):
        """åŸ·è¡Œ ETL Job, æ›´æ–°å¤±æ•—çµæœ, log å¤±æ•—è¨Šæ¯"""

        self.job_run.status = "failed"
        self.job_run.error_message = str(error)
        logger.exception(
            "ETL ä»»å‹™å¤±æ•—",
            extra={
                "event": "etl_failed",
                "job_run_id": self.job_run.id,
                "error": str(error),
            },
        )
        raise CommandError(f"åŸ·è¡Œå¤±æ•—: {error}")

    def _run_etl(self):
        """ETL ä¸»æµç¨‹"""
        # 1. Extract: ä¸‹è¼‰è³‡æ–™
        self.stdout.write("ğŸ“¥ éšæ®µ 1: æ“·å–è³‡æ–™...")
        data_chunks = self._extract_data()

        # 2. Transform & Load: æ¸…ç†ä¸¦è¼‰å…¥
        self.stdout.write("ğŸ”„ éšæ®µ 2: è½‰æ›ä¸¦è¼‰å…¥è³‡æ–™...")

        # å–å¾—èµ·å§‹æ‰¹æ¬¡(æ–·é»çºŒå‚³)
        start_batch = 1
        if self.resume:
            progress = (
                self._get_progress()
            )  # å¾—åˆ°ç‹€æ…‹åœç•™åœ¨ running ä¸­çš„ä»»å‹™çš„è¿½è¹¤é€²åº¦ instance
            if progress:
                start_batch = progress.last_successful_batch + 1
                self.stdout.write(f"  â© å¾æ‰¹æ¬¡ {start_batch} ç¹¼çºŒ...")

        # è™•ç†æ¯å€‹ chunk
        for chunk_num, df_chunk in enumerate(data_chunks, 1):
            # Garbage collection after every loop

            if chunk_num < start_batch:
                continue

            # é™åˆ¶è™•ç†ç­†æ•¸(for testing)
            self.stdout.write(f"ç›®å‰å·²è™•ç† {self.stats['total']}")
            if self.limit and self.stats["total"] >= self.limit:
                self.stdout.write(
                    self.style.WARNING(f"  å·²é”åˆ°é™åˆ¶ ({self.limit} ç­†),åœæ­¢è™•ç†")
                )
                break

            try:
                self._process_chunk(df_chunk, chunk_num)
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡ {chunk_num} è™•ç†å¤±æ•—: {e}")
                self._save_error_batch(df_chunk, chunk_num, str(e))

                # æ±ºå®šæ˜¯å¦ç¹¼çºŒ
                if not self._should_continue_on_error():
                    raise

    def _extract_data(self):
        """ä¸‹è¼‰ CSV è³‡æ–™"""
        session = self._create_session_with_retry()

        try:
            response = session.get(self.CSV_URL, stream=True, timeout=60)
            response.raise_for_status()

            # ä½¿ç”¨ Pandas åˆ†æ‰¹è®€å–
            return pd.read_csv(
                response.raw,
                encoding="utf-8",
                chunksize=self.chunk_size,
                dtype=str,  # å…¨éƒ¨å…ˆç•¶å­—ä¸²
                na_values=["", "NULL", "null", "NA", "N/A"],  # é€™äº›å€¼è¦–ç‚ºç©ºå€¼
                keep_default_na=False,  # ä¸è¦ç”¨ pandas é è¨­çš„ç©ºå€¼åˆ¤æ–·, "NA" â†’ NaNï¼ˆä½† "NA" å¯èƒ½æ˜¯å…¬å¸åç¨±çš„ä¸€éƒ¨åˆ†ï¼‰
            )  # å›å‚³ generator

        except requests.RequestException as e:
            raise CommandError(f"è³‡æ–™ä¸‹è¼‰å¤±æ•—: {e}")

    def _process_chunk(self, df_chunk: pd.DataFrame, chunk_num: int):
        """è™•ç†å–®ä¸€ chunk"""
        self.stdout.write(f"\nğŸ“¦ æ‰¹æ¬¡ {chunk_num}")
        self.stdout.write(f"  åŸå§‹ç­†æ•¸: {len(df_chunk):,}")

        logger.info(
            "é–‹å§‹è™•ç†æ‰¹æ¬¡",
            extra={
                "event": "batch_started",
                "job_run_id": self.job_run.id,
                "batch_num": chunk_num,
                "raw_count": len(df_chunk),
            },
        )
        # Transform: æ¸…ç†è³‡æ–™
        df_clean, errors = self._transform_data(df_chunk, chunk_num)
        self.stats["failed"] += len(errors)
        self.stats["total"] += len(df_chunk)

        # è¨˜éŒ„éŒ¯èª¤
        if errors:
            # If any row has error, record which batch it is in has error
            self._log_errors(errors, chunk_num)
            self.stdout.write(self.style.WARNING(f"  âš ï¸  é©—è­‰å¤±æ•—: {len(errors)} ç­†"))

            logger.warning(
                "æ‰¹æ¬¡é©—è­‰æœ‰éŒ¯èª¤",
                extra={
                    "event": "batch_validation_errors",
                    "job_run_id": self.job_run.id,
                    "batch_num": chunk_num,
                    "error_count": len(errors),
                },
            )
        # Load: è¼‰å…¥è³‡æ–™åº«
        if not df_clean.empty and not self.dry_run:
            success_count = self._load_data(df_clean, chunk_num)
            self.stats["success"] += success_count
            self.stdout.write(
                self.style.SUCCESS(f"  âœ… æˆåŠŸåŒ¯å…¥: {success_count:,} ç­†")
            )
            logger.info(
                "æ‰¹æ¬¡è™•ç†å®Œæˆ",
                extra={
                    "event": "batch_completed",
                    "job_run_id": self.job_run.id,
                    "batch_num": chunk_num,
                    "records_processed": success_count,
                },
            )
            # æ›´æ–°é€²åº¦
            self._update_progress(chunk_num)
        elif self.dry_run:
            self.stdout.write(
                self.style.NOTICE(f"  ğŸ” DRY RUN: å°‡åŒ¯å…¥ {len(df_clean):,} ç­†")
            )
            logger.info(
                "Dry run æ‰¹æ¬¡é è¦½",
                extra={
                    "event": "batch_dry_run",
                    "job_run_id": self.job_run.id,
                    "batch_num": chunk_num,
                    "would_process": len(df_clean),
                },
            )

    """
    ===== Transform ====
    """

    def _transform_data(
        self, df: pd.DataFrame, chunk_num: int
    ) -> Tuple[pd.DataFrame, List[dict]]:
        """æ¸…ç†ä¸¦é©—è­‰è³‡æ–™"""
        errors = []
        original_count = len(df)

        # 1. ç§»é™¤å®Œå…¨ç©ºç™½çš„è¡Œ
        df = df.dropna(how="all")

        # 2. é©—è­‰å¿…å¡«æ¬„ä½
        required_cols = ["çµ±ä¸€ç·¨è™Ÿ", "ç‡Ÿæ¥­äººåç¨±"]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logger.error(f"æ‰¹æ¬¡ {chunk_num} ç¼ºå°‘æ¬„ä½: {missing_cols}")
            logger.error(f"å¯¦éš›æ¬„ä½: {list(df.columns)}")
            logger.error(f"å‰ 3 è¡Œè³‡æ–™:\n{df.head(3)}")
            raise ValueError(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_cols}")

        # 3. æ¸…ç†çµ±ä¸€ç·¨è™Ÿ
        df["çµ±ä¸€ç·¨è™Ÿ"] = df["çµ±ä¸€ç·¨è™Ÿ"].fillna("").str.strip()

        # 4. é©—è­‰çµ±ä¸€ç·¨è™Ÿæ ¼å¼
        # Filter out the rows that is not digit and not 8 digits
        invalid_mask = (df["çµ±ä¸€ç·¨è™Ÿ"].str.len() != 8) | (~df["çµ±ä¸€ç·¨è™Ÿ"].str.isdigit())

        # è¨˜éŒ„æ ¼å¼éŒ¯èª¤
        # Loop for in invalid rows, df[invalid_mask] get us the entire invalid rows
        for _, row in df[invalid_mask].iterrows():
            errors.append(
                {
                    "type": "INVALID_BAN",
                    "batch": chunk_num,
                    "ban": row["çµ±ä¸€ç·¨è™Ÿ"],  # âœ… æ›´ç°¡æ½”
                    "message": f"çµ±ä¸€ç·¨è™Ÿæ ¼å¼éŒ¯èª¤: {row['çµ±ä¸€ç·¨è™Ÿ']}",
                }
            )

        df = df[~invalid_mask].copy()  # Filter out the actual valid rows

        # 5. è™•ç†é‡è¤‡è³‡æ–™
        duplicates_mask = df.duplicated(subset=["çµ±ä¸€ç·¨è™Ÿ"], keep="first")
        duplicates_count = duplicates_mask.sum()

        if duplicates_count > 0:
            self.stats["duplicates"] += duplicates_count
            for idx in df[duplicates_mask].index:
                errors.append(
                    {
                        "type": "DUPLICATE",
                        "batch": chunk_num,
                        "ban": df.loc[idx, "çµ±ä¸€ç·¨è™Ÿ"],
                        "message": f"é‡è¤‡çš„çµ±ä¸€ç·¨è™Ÿ: {df.loc[idx, 'çµ±ä¸€ç·¨è™Ÿ']}",
                    }
                )

        df = df[~duplicates_mask].copy()

        # 6. æ¸…ç†å…¶ä»–æ¬„ä½
        df = self._clean_fields(df)

        # Print out before and after rows
        cleaned_count = len(df)
        self.stdout.write(f"  æ¸…ç†: {original_count:,} â†’ {cleaned_count:,} ç­†")

        return df, errors

    def _clean_fields(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†å„æ¬„ä½"""
        # ç¸½æ©Ÿæ§‹çµ±ä¸€ç·¨è™Ÿ
        df["ç¸½æ©Ÿæ§‹çµ±ä¸€ç·¨è™Ÿ"] = df["ç¸½æ©Ÿæ§‹çµ±ä¸€ç·¨è™Ÿ"].fillna("").str.strip()
        df.loc[df["ç¸½æ©Ÿæ§‹çµ±ä¸€ç·¨è™Ÿ"] == "", "ç¸½æ©Ÿæ§‹çµ±ä¸€ç·¨è™Ÿ"] = None

        # å…¬å¸åç¨±
        df["ç‡Ÿæ¥­äººåç¨±"] = df["ç‡Ÿæ¥­äººåç¨±"].fillna("").str.strip()

        # åœ°å€
        df["ç‡Ÿæ¥­åœ°å€"] = df["ç‡Ÿæ¥­åœ°å€"].fillna("").str.strip()

        # è¨­ç«‹æ—¥æœŸ
        df["è¨­ç«‹æ—¥æœŸ"] = df["è¨­ç«‹æ—¥æœŸ"].fillna("").str.strip()

        # çµ„ç¹”åˆ¥
        df["çµ„ç¹”åˆ¥åç¨±"] = df["çµ„ç¹”åˆ¥åç¨±"].fillna("").str.strip()

        # è³‡æœ¬é¡
        df["è³‡æœ¬é¡"] = (
            pd.to_numeric(
                df["è³‡æœ¬é¡"].fillna("0").str.replace(",", ""),
                errors="coerce",  # if cannot convert turn it to NaN
            )
            .fillna(0)  # Turn NaN back to 0
            .astype("int64")
        )

        # ä½¿ç”¨çµ±ä¸€ç™¼ç¥¨
        df["ä½¿ç”¨çµ±ä¸€ç™¼ç¥¨"] = (
            df["ä½¿ç”¨çµ±ä¸€ç™¼ç¥¨"]
            .map({"Y": True, "y": True, "N": False, "n": False})
            .fillna(False)
        )

        # è¡Œæ¥­ä»£è™Ÿèˆ‡åç¨±
        for i in ["", "1", "2", "3"]:
            code_col = f"è¡Œæ¥­ä»£è™Ÿ{i}" if i else "è¡Œæ¥­ä»£è™Ÿ"
            name_col = f"åç¨±{i}" if i else "åç¨±"

            if code_col in df.columns:
                df[code_col] = df[code_col].fillna("").str.strip()
            if name_col in df.columns:
                df[name_col] = df[name_col].fillna("").str.strip()

        return df

    """
    ===== Load ====
    """

    def _load_data(self, df: pd.DataFrame, batch_num: int) -> int:
        """è¼‰å…¥è³‡æ–™åˆ°è³‡æ–™åº«"""
        try:
            with transaction.atomic():
                # æº–å‚™ TaxRegistration è³‡æ–™
                tax_records = self._prepare_tax_records(df)

                # ä½¿ç”¨ PostgreSQL COPY å¿«é€ŸåŒ¯å…¥
                count = self._bulk_insert_copy(tax_records)

                # åŒ¯å…¥è¡Œæ¥­è³‡æ–™
                industry_records = self._prepare_industry_records(df)
                if industry_records:
                    self._bulk_insert_industries(industry_records)

                return count

        except Exception as e:
            logger.error(f"æ‰¹æ¬¡ {batch_num} è¼‰å…¥å¤±æ•—: {e}")
            raise

    def _prepare_tax_records(self, df: pd.DataFrame) -> pd.DataFrame:
        """æº–å‚™ TaxRegistration è¨˜éŒ„"""
        return df[
            [
                "çµ±ä¸€ç·¨è™Ÿ",
                "ç¸½æ©Ÿæ§‹çµ±ä¸€ç·¨è™Ÿ",
                "ç‡Ÿæ¥­äººåç¨±",
                "ç‡Ÿæ¥­åœ°å€",
                "è³‡æœ¬é¡",
                "è¨­ç«‹æ—¥æœŸ",
                "çµ„ç¹”åˆ¥åç¨±",
                "ä½¿ç”¨çµ±ä¸€ç™¼ç¥¨",
            ]
        ].copy()  # Select only these columns as sheet

    def _bulk_insert_copy(self, df: pd.DataFrame) -> int:
        """ä½¿ç”¨ PostgreSQL COPY æ‰¹æ¬¡åŒ¯å…¥"""
        # è½‰æ›ç‚º CSV StringIO
        buffer = StringIO()

        # æº–å‚™æ¬„ä½é †åº
        df_ordered = df.copy()

        # è™•ç† NULL å€¼
        df_ordered["ç¸½æ©Ÿæ§‹çµ±ä¸€ç·¨è™Ÿ"] = df_ordered["ç¸½æ©Ÿæ§‹çµ±ä¸€ç·¨è™Ÿ"].replace(
            {None: "\\N", "": "\\N"}
        )

        # å¯«å…¥ CSV
        df_ordered.to_csv(
            buffer,
            index=False,
            header=False,
            sep="\t",
            na_rep="\\N",
            quoting=csv.QUOTE_MINIMAL,  # âœ… åŠ å…¥é€™å€‹
            escapechar="\\",  # âœ… åŠ å…¥é€™å€‹
            doublequote=False,
        )
        buffer.seek(0)

        # COPY åŒ¯å…¥
        with connection.cursor() as cursor:
            cursor.copy_from(
                buffer,
                "tax_registration",  # è¡¨å
                sep="\t",
                null="\\N",
                columns=(
                    "ban",
                    "headquarters_ban",
                    "business_name",
                    "business_address",
                    "capital_amount",
                    "business_setup_date",
                    "business_type",
                    "is_use_invoice",
                ),
            )

        return len(df_ordered)

    def _prepare_industry_records(self, df: pd.DataFrame) -> List[BusinessIndustry]:
        """æº–å‚™è¡Œæ¥­è¨˜éŒ„ for bulk insert"""
        records = []

        for _, row in df.iterrows():
            ban = row["çµ±ä¸€ç·¨è™Ÿ"]

            # è™•ç†æœ€å¤š 4 çµ„è¡Œæ¥­
            for i, suffix in enumerate(["", "1", "2", "3"], 1):
                code_col = f"è¡Œæ¥­ä»£è™Ÿ{suffix}" if suffix else "è¡Œæ¥­ä»£è™Ÿ"
                name_col = f"åç¨±{suffix}" if suffix else "åç¨±"

                if code_col in row and row[code_col]:
                    records.append(
                        BusinessIndustry(
                            business_id=ban,
                            industry_code=row[code_col],
                            industry_name=row.get(name_col, ""),
                            order=i,
                        )
                    )

        return records

    def _bulk_insert_industries(self, records: List[BusinessIndustry]):
        """æ‰¹æ¬¡åŒ¯å…¥è¡Œæ¥­è³‡æ–™"""
        # ä½¿ç”¨ bulk_create æ­é… ignore_conflicts
        BusinessIndustry.objects.bulk_create(
            records,
            batch_size=5000,
            ignore_conflicts=True,  # å¿½ç•¥é‡è¤‡çš„è¡Œæ¥­ä»£è™Ÿ
        )

    def _log_errors(self, errors: List[dict], chunk_num: int):
        """è¨˜éŒ„éŒ¯èª¤åˆ°è³‡æ–™åº«"""
        error_records = [
            DataImportError(
                job_run=self.job_run,
                batch_number=chunk_num,
                error_type=err["type"],
                error_message=err["message"],
                raw_data=err,
            )
            for err in errors[
                :100
            ]  # Limit recording 100 errors to prevent stressful db write
        ]  # If it's over 100 error then it indicates the raw data is problematic

        DataImportError.objects.bulk_create(error_records, ignore_conflicts=True)

    def _update_progress(self, batch_num: int):
        """æ›´æ–°é€²åº¦"""
        progress, created = ImportProgress.objects.get_or_create(
            job_run=self.job_run, defaults={"total_batches": 0}
        )

        progress.last_successful_batch = batch_num
        progress.current_batch = batch_num
        progress.save()

    def _get_progress(self) -> ImportProgress:
        """å–å¾—é€²åº¦(æ–·é»çºŒå‚³)"""
        try:
            # æ‰¾æœ€è¿‘ä¸€æ¬¡æœªå®Œæˆçš„åŸ·è¡Œ
            last_job = (
                ETLJobRun.objects.filter(status="running")
                .order_by("-started_at")
                .first()
            )

            if last_job:
                return ImportProgress.objects.filter(job_run=last_job).first()
        except Exception:
            pass
        return None

    def _save_error_batch(self, df: pd.DataFrame, batch_num: int, error: str):
        """å„²å­˜éŒ¯èª¤æ‰¹æ¬¡è³‡æ–™"""
        error_file = (
            f"./errors/error_batch_{batch_num}_{datetime.now():%Y%m%d_%H%M%S}.csv"
        )
        file_path = Path(error_file)
        file_path.parent.mkdir(parents=True, exist_ok=True)

        df.to_csv(error_file, index=False, encoding="utf-8-sig")
        self.stderr.write(self.style.ERROR(f"  éŒ¯èª¤è³‡æ–™å·²å„²å­˜: {error_file}"))

    def _print_summary(self):
        """è¼¸å‡ºåŸ·è¡Œæ‘˜è¦"""
        duration = self.job_run.duration_seconds or 0
        success_rate = self.job_run.success_rate

        self.stdout.write(
            self.style.MIGRATE_HEADING(f"\n{'=' * 60}\nåŸ·è¡Œæ‘˜è¦\n{'=' * 60}\n")
        )

        self.stdout.write(f"åŸ·è¡Œ ID:      {self.job_run.id}")
        self.stdout.write(f"ç‹€æ…‹:         {self.job_run.get_status_display()}")
        self.stdout.write(f"åŸ·è¡Œæ™‚é–“:     {duration:.2f} ç§’")
        self.stdout.write("\nè™•ç†çµ±è¨ˆ:")
        self.stdout.write(f"  ç¸½ç­†æ•¸:     {self.stats['total']:,}")
        self.stdout.write(
            self.style.SUCCESS(
                f"  âœ… æˆåŠŸ:    {self.stats['success']:,} ({success_rate:.2f}%)"
            )
        )

        if self.stats["failed"] > 0:
            self.stdout.write(
                self.style.ERROR(f"  âŒ å¤±æ•—:    {self.stats['failed']:,}")
            )

        if self.stats["duplicates"] > 0:
            self.stdout.write(
                self.style.WARNING(f"  ğŸ”„ é‡è¤‡:    {self.stats['duplicates']:,}")
            )

        self.stdout.write(f"\n{'=' * 60}\n")

        # æç¤ºæŸ¥çœ‹è©³ç´°éŒ¯èª¤
        if self.stats["failed"] > 0:
            self.stdout.write(
                self.style.NOTICE(
                    f"\nğŸ’¡ æŸ¥çœ‹è©³ç´°éŒ¯èª¤:\n"
                    f"   python manage.py shell\n"
                    f"   >>> from tax_registration.models import DataImportError\n"
                    f"   >>> DataImportError.objects.filter(job_run_id={self.job_run.id})\n"
                )
            )
        # === çµæ§‹åŒ– log çµ¦ CloudWatch ===
        logger.info(
            "ETL åŸ·è¡Œæ‘˜è¦",
            extra={
                "event": "etl_summary",
                "job_run_id": self.job_run.id,
                "status": self.job_run.status,
                "duration_seconds": round(duration, 2),
                "success_rate": round(success_rate, 2),
                "records_total": self.stats["total"],
                "records_success": self.stats["success"],
                "records_failed": self.stats["failed"],
                "records_duplicates": self.stats["duplicates"],
            },
        )

    def _create_session_with_retry(self) -> requests.Session:
        """å»ºç«‹å¸¶é‡è©¦æ©Ÿåˆ¶çš„ Session"""
        # Create longer connection
        session = requests.Session()

        retry = Retry(
            total=3,  # æœ€å¤šé‡è©¦ 3 æ¬¡
            backoff_factor=1,  # æ¯æ¬¡ç­‰å¾…æ™‚é–“ï¼š1ç§’ â†’ 2ç§’ â†’ 4ç§’ï¼ˆæŒ‡æ•¸éå¢ï¼‰
            status_forcelist=[429, 500, 502, 503, 504],  # é‡åˆ°é€™äº›éŒ¯èª¤ç¢¼æ‰é‡è©¦
            allowed_methods=["GET"],  # åªæœ‰ GET è«‹æ±‚æ‰é‡è©¦ï¼ˆä¸é‡è©¦ POST/PUTï¼‰
        )

        adapter = HTTPAdapter(max_retries=retry)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        return session

    def _confirm_truncate(self) -> bool:
        """ç¢ºèªæ¸…ç©ºè³‡æ–™"""
        count = TaxRegistration.objects.count()
        self.stdout.write(
            self.style.WARNING(f"\nâš ï¸  è­¦å‘Š:å³å°‡åˆªé™¤ {count:,} ç­†ç‡Ÿæ¥­ç™»è¨˜è³‡æ–™!")
        )

        answer = input("ç¢ºå®šè¦ç¹¼çºŒå—? (yes/no): ")
        return answer.lower() == "yes"

    def _truncate_tables(self):
        """æ¸…ç©ºè³‡æ–™è¡¨"""
        self.stdout.write("ğŸ—‘ï¸  æ¸…ç©ºè³‡æ–™è¡¨...")

        # Faster deletion
        with connection.cursor() as cursor:
            cursor.execute("TRUNCATE TABLE tax_registration CASCADE;")
            cursor.execute("TRUNCATE TABLE business_industry CASCADE;")

        self.stdout.write(self.style.SUCCESS("  âœ… å®Œæˆ"))

    def _should_continue_on_error(self) -> bool:
        """è©¢å•æ˜¯å¦ç¹¼çºŒ"""
        if self.dry_run:
            return True

        answer = input("\nç™¼ç”ŸéŒ¯èª¤,æ˜¯å¦ç¹¼çºŒä¸‹ä¸€æ‰¹æ¬¡? (yes/no): ")
        return answer.lower() == "yes"
