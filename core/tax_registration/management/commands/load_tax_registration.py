# tax_registration/management/commands/import_business.py
import logging
from pathlib import Path
import pandas as pd
import requests
from contextlib import contextmanager
from typing import List
from datetime import datetime

from django.core.management.base import BaseCommand, CommandError
from django.db import connection
from django.utils import timezone

from core.tax_registration.models import (
    TaxRegistration,
    ETLJobRun,
    DataImportError,
    ImportProgress,
)

from core.tax_registration.etl.extractor import CSVExtractor
from core.tax_registration.etl.transformer import TaxDataTransformer
from core.tax_registration.etl.loader import BulkLoader


logger = logging.getLogger("tax_registration.etl")


class Command(BaseCommand):
    help = "åŒ¯å…¥å…¨åœ‹ç‡Ÿæ¥­ç™»è¨˜è³‡æ–™(å„ªåŒ–ç‰ˆ)"

    # é¡åˆ¥å¸¸æ•¸
    CSV_URL = "https://eip.fia.gov.tw/data/BGMOPEN1.csv"

    def __init__(self):
        super().__init__()
        self.stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "duplicates": 0,
            "skipped": 0,
        }
        self.job_run = None
        self.start_time = None

    def add_arguments(self, parser):
        parser.add_argument(
            "--batch-size",
            type=int,
            default=10000,
            help="æ¯æ‰¹æ¬¡è™•ç†ç­†æ•¸(å»ºè­° 5000-20000)",
        )
        parser.add_argument(
            "--chunk-size", type=int, default=50000, help="CSV è®€å– chunk å¤§å°"
        )
        parser.add_argument("--resume", action="store_true", help="å¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒ")
        parser.add_argument(
            "--dry-run", action="store_true", help="åªé©—è­‰è³‡æ–™ä¸å¯¦éš›åŒ¯å…¥"
        )
        parser.add_argument(
            "--truncate", action="store_true", help="æ¸…ç©ºç¾æœ‰è³‡æ–™å¾Œé‡æ–°åŒ¯å…¥(å±éšªæ“ä½œ!)"
        )
        parser.add_argument(
            "--limit", type=int, default=None, help="é™åˆ¶è™•ç†ç­†æ•¸(æ¸¬è©¦ç”¨)"
        )
        parser.add_argument(
            "--auto", action="store_true", help="è·³éç¢ºèªæç¤ºï¼ˆç”¨æ–¼è‡ªå‹•åŒ–æ’ç¨‹ï¼‰"
        )

    def handle(self, *args, **options):
        """ä¸»è¦é€²å…¥é»"""
        self.batch_size = options["batch_size"]
        self.chunk_size = options["chunk_size"]
        self.dry_run = options["dry_run"]
        self.resume = options["resume"]
        self.limit = options["limit"]
        self.auto = options["auto"]  # å­˜èµ·ä¾†

        # æª¢æŸ¥æ˜¯å¦æœ‰æ­£åœ¨åŸ·è¡Œä¸­çš„ä»»å‹™
        ongoing_job = ETLJobRun.objects.filter(
            status="running"
        ).exists()  # Already indexed, the query is fast

        if ongoing_job:
            self.stdout.write(self.style.ERROR("å·²æœ‰ä»»å‹™æ­£åœ¨åŸ·è¡Œä¸­ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"))
            return  # çµ‚æ­¢åŸ·è¡Œ

        if options["truncate"] and options["resume"]:
            raise CommandError(
                "âŒ --truncate å’Œ --resume ä¸èƒ½åŒæ™‚ä½¿ç”¨\n"
                "   --truncate: æ¸…ç©ºè³‡æ–™å¾Œé‡æ–°åŒ¯å…¥\n"
                "   --resume: å¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒ"
            )

        # è¼‰å…¥æ–°è³‡æ–™å‰ç©º table
        if options["truncate"]:
            if not self._confirm_truncate():
                self.stdout.write(self.style.WARNING("æ“ä½œå·²å–æ¶ˆ"))
                return
            self._truncate_tables()

        # å»ºç«‹åŸ·è¡Œç´€éŒ„
        self.create_etl_job()

        try:
            self.handle_successful_etl_job()
        except Exception as e:
            self.handle_failed_etl_job(e)
        finally:
            self.job_run.completed_at = timezone.now()
            self.job_run.save()
            self._print_summary()

    def create_etl_job(self):
        """å‰µå»º ETL Job ä¸¦ log"""
        self.job_run = ETLJobRun.objects.create(
            status="running",
            batch_size=self.batch_size,
            chunk_size=self.chunk_size,
            data_source_url=self.CSV_URL,
        )
        logger.info(
            "ETL ä»»å‹™é–‹å§‹",
            extra={
                "event": "etl_started",
                "job_run_id": self.job_run.id,
                "batch_size": self.batch_size,
                "chunk_size": self.chunk_size,
                "dry_run": self.dry_run,
            },
        )

    def handle_successful_etl_job(self):
        """åŸ·è¡Œ ETL Job, æ›´æ–°æˆåŠŸçµæœ, log æˆåŠŸè¨Šæ¯"""
        with self._track_progress():
            self._run_etl()

        # æ›´æ–°åŸ·è¡Œçµæœ
        self.job_run.status = "success"
        self.job_run.records_total = self.stats["total"]
        self.job_run.records_processed = self.stats["success"]
        self.job_run.records_failed = self.stats["failed"]
        self.job_run.records_duplicated = self.stats["duplicates"]

        logger.info(
            "ETL ä»»å‹™å®Œæˆ",
            extra={
                "event": "etl_completed",
                "job_run_id": self.job_run.id,
                "status": "success",
                "records_total": self.stats["total"],
                "records_processed": self.stats["success"],
                "records_failed": self.stats["failed"],
            },
        )

    @contextmanager
    def _track_progress(self):
        """è¿½è¹¤åŸ·è¡Œé€²åº¦"""
        self.start_time = timezone.now()
        self.stdout.write(
            self.style.MIGRATE_HEADING(
                f"\n{'=' * 60}\né–‹å§‹åŸ·è¡Œ ETL (ID: {self.job_run.id})\n{'=' * 60}\n"
            )
        )
        yield  # _run_etl runs here
        duration = (timezone.now() - self.start_time).total_seconds()
        self.stdout.write(self.style.SUCCESS(f"\nåŸ·è¡Œæ™‚é–“: {duration:.2f} ç§’"))

    def handle_failed_etl_job(self, error):
        """åŸ·è¡Œ ETL Job, æ›´æ–°å¤±æ•—çµæœ, log å¤±æ•—è¨Šæ¯"""

        self.job_run.status = "failed"
        self.job_run.error_message = str(error)
        logger.exception(
            "ETL ä»»å‹™å¤±æ•—",
            extra={
                "event": "etl_failed",
                "job_run_id": self.job_run.id,
                "error": str(error),
            },
        )
        raise CommandError(f"åŸ·è¡Œå¤±æ•—: {error}")

    def _run_etl(self):
        """ETL ä¸»æµç¨‹"""
        # 1. Extract: ä¸‹è¼‰è³‡æ–™
        self.stdout.write("ğŸ“¥ éšæ®µ 1: æ“·å–è³‡æ–™...")

        try:
            extractor = CSVExtractor(self.CSV_URL)
            data_chunks = extractor.fetch_chunks(self.chunk_size)
        except requests.RequestException as e:
            raise CommandError(f"è³‡æ–™ä¸‹è¼‰å¤±æ•—: {e}")

        # 2. Transform & Load: æ¸…ç†ä¸¦è¼‰å…¥
        self.transformer = TaxDataTransformer()
        self.loader = BulkLoader(self.batch_size)

        self.stdout.write("ğŸ”„ éšæ®µ 2: è½‰æ›ä¸¦è¼‰å…¥è³‡æ–™...")

        # å–å¾—èµ·å§‹æ‰¹æ¬¡(æ–·é»çºŒå‚³)
        start_batch = 1
        if self.resume:
            progress = (
                self._get_progress()
            )  # å¾—åˆ°ç‹€æ…‹åœç•™åœ¨ running ä¸­çš„ä»»å‹™çš„è¿½è¹¤é€²åº¦ instance
            if progress:
                start_batch = progress.last_successful_batch + 1
                self.stdout.write(f"  â© å¾æ‰¹æ¬¡ {start_batch} ç¹¼çºŒ...")

        # è™•ç†æ¯å€‹ chunk
        for chunk_num, df_chunk in enumerate(data_chunks, 1):
            # Garbage collection after every loop

            if chunk_num < start_batch:
                continue

            # é™åˆ¶è™•ç†ç­†æ•¸(for testing)
            self.stdout.write(f"ç›®å‰å·²è™•ç† {self.stats['total']}")
            if self.limit and self.stats["total"] >= self.limit:
                self.stdout.write(
                    self.style.WARNING(f"  å·²é”åˆ°é™åˆ¶ ({self.limit} ç­†),åœæ­¢è™•ç†")
                )
                break

            try:
                self._process_chunk(df_chunk, chunk_num)
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡ {chunk_num} è™•ç†å¤±æ•—: {e}")
                self._save_error_batch(df_chunk, chunk_num, str(e))

                # æ±ºå®šæ˜¯å¦ç¹¼çºŒ
                if not self._should_continue_on_error():
                    raise

    def _process_chunk(self, df_chunk: pd.DataFrame, chunk_num: int):
        """è™•ç†å–®ä¸€ chunk"""
        original_count = len(df_chunk)

        self.stdout.write(f"\nğŸ“¦ æ‰¹æ¬¡ {chunk_num}")
        self.stdout.write(f"  åŸå§‹ç­†æ•¸: {original_count:,}")

        logger.info(
            "é–‹å§‹è™•ç†æ‰¹æ¬¡",
            extra={
                "event": "batch_started",
                "job_run_id": self.job_run.id,
                "batch_num": chunk_num,
                "raw_count": original_count,
            },
        )
        # Transform: æ¸…ç†è³‡æ–™
        df_clean, errors = self.transformer.process(df_chunk, chunk_num)
        self.stdout.write(f"  æ¸…ç†: {original_count:,} â†’ {len(df_clean):,} ç­†")

        self.stats["failed"] += len(errors)
        self.stats["total"] += original_count

        # çµ±è¨ˆé‡è¤‡ç­†æ•¸
        duplicates_count = sum(1 for e in errors if e["type"] == "DUPLICATE")
        self.stats["duplicates"] += duplicates_count

        # è¨˜éŒ„éŒ¯èª¤
        if errors:
            # If any row has error, record which batch it is in has error
            self._log_errors(errors, chunk_num)
            self.stdout.write(self.style.WARNING(f"  âš ï¸  é©—è­‰å¤±æ•—: {len(errors)} ç­†"))

            logger.warning(
                "æ‰¹æ¬¡é©—è­‰æœ‰éŒ¯èª¤",
                extra={
                    "event": "batch_validation_errors",
                    "job_run_id": self.job_run.id,
                    "batch_num": chunk_num,
                    "error_count": len(errors),
                },
            )
        # Load: è¼‰å…¥è³‡æ–™åº«
        if not df_clean.empty and not self.dry_run:
            success_count = self.loader.insert(df_clean)
            self.stats["success"] += success_count
            self.stdout.write(
                self.style.SUCCESS(f"  âœ… æˆåŠŸåŒ¯å…¥: {success_count:,} ç­†")
            )
            logger.info(
                "æ‰¹æ¬¡è™•ç†å®Œæˆ",
                extra={
                    "event": "batch_completed",
                    "job_run_id": self.job_run.id,
                    "batch_num": chunk_num,
                    "records_processed": success_count,
                },
            )
            # æ›´æ–°é€²åº¦
            self._update_progress(chunk_num)
        elif self.dry_run:
            self.stdout.write(
                self.style.NOTICE(f"  ğŸ” DRY RUN: å°‡åŒ¯å…¥ {len(df_clean):,} ç­†")
            )
            logger.info(
                "Dry run æ‰¹æ¬¡é è¦½",
                extra={
                    "event": "batch_dry_run",
                    "job_run_id": self.job_run.id,
                    "batch_num": chunk_num,
                    "would_process": len(df_clean),
                },
            )

    """
    ===== Load ====
    """

    def _log_errors(self, errors: List[dict], chunk_num: int):
        """è¨˜éŒ„éŒ¯èª¤åˆ°è³‡æ–™åº«"""
        error_records = [
            DataImportError(
                job_run=self.job_run,
                batch_number=chunk_num,
                error_type=err["type"],
                error_message=err["message"],
                raw_data=err,
            )
            for err in errors[
                :100
            ]  # Limit recording 100 errors to prevent stressful db write
        ]  # If it's over 100 error then it indicates the raw data is problematic

        DataImportError.objects.bulk_create(error_records, ignore_conflicts=True)

    def _update_progress(self, batch_num: int):
        """æ›´æ–°é€²åº¦"""
        progress, created = ImportProgress.objects.get_or_create(
            job_run=self.job_run, defaults={"total_batches": 0}
        )

        progress.last_successful_batch = batch_num
        progress.current_batch = batch_num
        progress.save()

    def _get_progress(self) -> ImportProgress:
        """å–å¾—é€²åº¦(æ–·é»çºŒå‚³)"""
        try:
            # æ‰¾æœ€è¿‘ä¸€æ¬¡æœªå®Œæˆçš„åŸ·è¡Œ
            last_job = (
                ETLJobRun.objects.filter(status="running")
                .order_by("-started_at")
                .first()
            )

            if last_job:
                return ImportProgress.objects.filter(job_run=last_job).first()
        except Exception:
            pass
        return None

    def _save_error_batch(self, df: pd.DataFrame, batch_num: int, error: str):
        """å„²å­˜éŒ¯èª¤æ‰¹æ¬¡è³‡æ–™"""
        error_file = (
            f"./errors/error_batch_{batch_num}_{datetime.now():%Y%m%d_%H%M%S}.csv"
        )
        file_path = Path(error_file)
        file_path.parent.mkdir(parents=True, exist_ok=True)

        df.to_csv(error_file, index=False, encoding="utf-8-sig")
        self.stderr.write(self.style.ERROR(f"  éŒ¯èª¤è³‡æ–™å·²å„²å­˜: {error_file}"))

    def _print_summary(self):
        """è¼¸å‡ºåŸ·è¡Œæ‘˜è¦"""
        duration = self.job_run.duration_seconds or 0
        success_rate = self.job_run.success_rate

        self.stdout.write(
            self.style.MIGRATE_HEADING(f"\n{'=' * 60}\nåŸ·è¡Œæ‘˜è¦\n{'=' * 60}\n")
        )

        self.stdout.write(f"åŸ·è¡Œ ID:      {self.job_run.id}")
        self.stdout.write(f"ç‹€æ…‹:         {self.job_run.get_status_display()}")
        self.stdout.write(f"åŸ·è¡Œæ™‚é–“:     {duration:.2f} ç§’")
        self.stdout.write("\nè™•ç†çµ±è¨ˆ:")
        self.stdout.write(f"  ç¸½ç­†æ•¸:     {self.stats['total']:,}")
        self.stdout.write(
            self.style.SUCCESS(
                f"  âœ… æˆåŠŸ:    {self.stats['success']:,} ({success_rate:.2f}%)"
            )
        )

        if self.stats["failed"] > 0:
            self.stdout.write(
                self.style.ERROR(f"  âŒ å¤±æ•—:    {self.stats['failed']:,}")
            )

        if self.stats["duplicates"] > 0:
            self.stdout.write(
                self.style.WARNING(f"  ğŸ”„ é‡è¤‡:    {self.stats['duplicates']:,}")
            )

        self.stdout.write(f"\n{'=' * 60}\n")

        # æç¤ºæŸ¥çœ‹è©³ç´°éŒ¯èª¤
        if self.stats["failed"] > 0:
            self.stdout.write(
                self.style.NOTICE(
                    f"\nğŸ’¡ æŸ¥çœ‹è©³ç´°éŒ¯èª¤:\n"
                    f"   python manage.py shell\n"
                    f"   >>> from tax_registration.models import DataImportError\n"
                    f"   >>> DataImportError.objects.filter(job_run_id={self.job_run.id})\n"
                )
            )
        # === çµæ§‹åŒ– log çµ¦ CloudWatch ===
        logger.info(
            "ETL åŸ·è¡Œæ‘˜è¦",
            extra={
                "event": "etl_summary",
                "job_run_id": self.job_run.id,
                "status": self.job_run.status,
                "duration_seconds": round(duration, 2),
                "success_rate": round(success_rate, 2),
                "records_total": self.stats["total"],
                "records_success": self.stats["success"],
                "records_failed": self.stats["failed"],
                "records_duplicates": self.stats["duplicates"],
            },
        )

    def _confirm_truncate(self) -> bool:
        """ç¢ºèªæ¸…ç©ºè³‡æ–™"""
        if self.auto:
            return True  # è‡ªå‹•åŒ–æ¨¡å¼ç›´æ¥é€šé

        count = TaxRegistration.objects.count()
        self.stdout.write(
            self.style.WARNING(f"\nâš ï¸  è­¦å‘Š:å³å°‡åˆªé™¤ {count:,} ç­†ç‡Ÿæ¥­ç™»è¨˜è³‡æ–™!")
        )

        answer = input("ç¢ºå®šè¦ç¹¼çºŒå—? (yes/no): ")
        return answer.lower() == "yes"

    def _truncate_tables(self):
        """æ¸…ç©ºè³‡æ–™è¡¨"""
        self.stdout.write("ğŸ—‘ï¸  æ¸…ç©ºè³‡æ–™è¡¨...")

        # Faster deletion
        with connection.cursor() as cursor:
            cursor.execute("TRUNCATE TABLE tax_registration CASCADE;")
            cursor.execute("TRUNCATE TABLE business_industry CASCADE;")

        self.stdout.write(self.style.SUCCESS("  âœ… å®Œæˆ"))

    def _should_continue_on_error(self) -> bool:
        """è©¢å•æ˜¯å¦ç¹¼çºŒ"""
        if self.dry_run:
            return True

        answer = input("\nç™¼ç”ŸéŒ¯èª¤,æ˜¯å¦ç¹¼çºŒä¸‹ä¸€æ‰¹æ¬¡? (yes/no): ")
        return answer.lower() == "yes"
