# tax_registration/management/commands/import_business.py
import logging
import pandas as pd
import requests
from contextlib import contextmanager

from django.core.management.base import BaseCommand, CommandError
from django.db import connection
from django.utils import timezone

from core.tax_registration.models import (
    TaxRegistration,
    ETLJobRun,
)

from core.tax_registration.etl.extractor import CSVExtractor
from core.tax_registration.etl.transformer import TaxDataTransformer
from core.tax_registration.etl.loader import BulkLoader
from core.tax_registration.etl.tracker import ETLTracker


logger = logging.getLogger("tax_registration.etl")


class Command(BaseCommand):
    help = "åŒ¯å…¥å…¨åœ‹ç‡Ÿæ¥­ç™»è¨˜è³‡æ–™(å„ªåŒ–ç‰ˆ)"

    # é¡åˆ¥å¸¸æ•¸
    CSV_URL = "https://eip.fia.gov.tw/data/BGMOPEN1.csv"

    def __init__(self):
        super().__init__()
        self.tracker = None

    def add_arguments(self, parser):
        parser.add_argument(
            "--batch-size",
            type=int,
            default=10000,
            help="æ¯æ‰¹æ¬¡è™•ç†ç­†æ•¸(å»ºè­° 5000-20000)",
        )
        parser.add_argument(
            "--chunk-size", type=int, default=50000, help="CSV è®€å– chunk å¤§å°"
        )
        parser.add_argument("--resume", action="store_true", help="å¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒ")
        parser.add_argument(
            "--dry-run", action="store_true", help="åªé©—è­‰è³‡æ–™ä¸å¯¦éš›åŒ¯å…¥"
        )
        parser.add_argument(
            "--truncate", action="store_true", help="æ¸…ç©ºç¾æœ‰è³‡æ–™å¾Œé‡æ–°åŒ¯å…¥(å±éšªæ“ä½œ!)"
        )
        parser.add_argument(
            "--limit", type=int, default=None, help="é™åˆ¶è™•ç†ç­†æ•¸(æ¸¬è©¦ç”¨)"
        )
        parser.add_argument(
            "--auto", action="store_true", help="è·³éç¢ºèªæç¤ºï¼ˆç”¨æ–¼è‡ªå‹•åŒ–æ’ç¨‹ï¼‰"
        )

    def handle(self, *args, **options):
        """ä¸»è¦é€²å…¥é»"""
        self.batch_size = options["batch_size"]
        self.chunk_size = options["chunk_size"]
        self.dry_run = options["dry_run"]
        self.resume = options["resume"]
        self.limit = options["limit"]
        self.auto = options["auto"]  # å­˜èµ·ä¾†

        # æª¢æŸ¥æ˜¯å¦æœ‰æ­£åœ¨åŸ·è¡Œä¸­çš„ä»»å‹™
        ongoing_job = ETLJobRun.objects.filter(
            status="running"
        ).exists()  # Already indexed, the query is fast

        if ongoing_job:
            self.stdout.write(self.style.ERROR("å·²æœ‰ä»»å‹™æ­£åœ¨åŸ·è¡Œä¸­ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"))
            return  # çµ‚æ­¢åŸ·è¡Œ

        if options["truncate"] and options["resume"]:
            raise CommandError(
                "âŒ --truncate å’Œ --resume ä¸èƒ½åŒæ™‚ä½¿ç”¨\n"
                "   --truncate: æ¸…ç©ºè³‡æ–™å¾Œé‡æ–°åŒ¯å…¥\n"
                "   --resume: å¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒ"
            )

        # è¼‰å…¥æ–°è³‡æ–™å‰ç©º table
        if options["truncate"]:
            if not self._confirm_truncate():
                self.stdout.write(self.style.WARNING("æ“ä½œå·²å–æ¶ˆ"))
                return
            self._truncate_tables()

        # å»ºç«‹åŸ·è¡Œç´€éŒ„
        self.tracker = ETLTracker(
            batch_size=self.batch_size,
            chunk_size=self.chunk_size,
            data_source_url=self.CSV_URL,
            dry_run=self.dry_run,
        )
        self.tracker.start()

        try:
            raise Exception("Test for fail...!")
            self.handle_successful_etl_job()
        except Exception as e:
            self.handle_failed_etl_job(e)
        finally:
            self._print_summary()

    def handle_successful_etl_job(self):
        """åŸ·è¡Œ ETL Job, æ›´æ–°æˆåŠŸçµæœ, log æˆåŠŸè¨Šæ¯"""
        with self._track_progress():
            self._run_etl()
        self.tracker.complete()

    @contextmanager
    def _track_progress(self):
        """è¿½è¹¤åŸ·è¡Œé€²åº¦"""
        self.stdout.write(
            self.style.MIGRATE_HEADING(
                f"\n{'=' * 60}\né–‹å§‹åŸ·è¡Œ ETL (ID: {self.tracker.job_run.id})\n{'=' * 60}\n"
            )
        )
        yield  # _run_etl runs here
        duration = (timezone.now() - self.tracker.start_time).total_seconds()
        self.stdout.write(self.style.SUCCESS(f"\nåŸ·è¡Œæ™‚é–“: {duration:.2f} ç§’"))

    def handle_failed_etl_job(self, error):
        """åŸ·è¡Œ ETL Job, æ›´æ–°å¤±æ•—çµæœ, log å¤±æ•—è¨Šæ¯"""

        self.tracker.fail(error)

        raise CommandError(f"åŸ·è¡Œå¤±æ•—: {error}")

    def _run_etl(self):
        """ETL ä¸»æµç¨‹"""
        # 1. Extract: ä¸‹è¼‰è³‡æ–™
        self.stdout.write("ğŸ“¥ éšæ®µ 1: æ“·å–è³‡æ–™...")

        try:
            extractor = CSVExtractor(self.CSV_URL)
            data_chunks = extractor.fetch_chunks(self.chunk_size)
        except requests.RequestException as e:
            raise CommandError(f"è³‡æ–™ä¸‹è¼‰å¤±æ•—: {e}")

        # 2. Transform & Load: æ¸…ç†ä¸¦è¼‰å…¥
        self.transformer = TaxDataTransformer()
        self.loader = BulkLoader(self.batch_size)

        self.stdout.write("ğŸ”„ éšæ®µ 2: è½‰æ›ä¸¦è¼‰å…¥è³‡æ–™...")

        # å–å¾—èµ·å§‹æ‰¹æ¬¡(æ–·é»çºŒå‚³)
        start_batch = 1
        if self.resume:
            start_batch = self.tracker.get_resume_batch()

            if start_batch > 1:
                self.stdout.write(f"  â© å¾æ‰¹æ¬¡ {start_batch} ç¹¼çºŒ...")

        # è™•ç†æ¯å€‹ chunk
        for chunk_num, df_chunk in enumerate(data_chunks, 1):
            # Garbage collection after every loop

            if chunk_num < start_batch:
                continue

            # é™åˆ¶è™•ç†ç­†æ•¸(for testing)
            self.stdout.write(f"ç›®å‰å·²è™•ç† {self.tracker.stats['total']}")
            if self.limit and self.tracker.stats["total"] >= self.limit:
                self.stdout.write(
                    self.style.WARNING(f"  å·²é”åˆ°é™åˆ¶ ({self.limit} ç­†),åœæ­¢è™•ç†")
                )
                break

            try:
                self._process_chunk(df_chunk, chunk_num)
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡ {chunk_num} è™•ç†å¤±æ•—: {e}")
                self.tracker.save_error_batch(df_chunk, chunk_num, str(e))

                # æ±ºå®šæ˜¯å¦ç¹¼çºŒ
                if not self._should_continue_on_error():
                    raise

    def _process_chunk(self, df_chunk: pd.DataFrame, chunk_num: int):
        """è™•ç†å–®ä¸€ chunk"""
        original_count = len(df_chunk)

        self.stdout.write(f"\nğŸ“¦ æ‰¹æ¬¡ {chunk_num}")
        self.stdout.write(f"  åŸå§‹ç­†æ•¸: {original_count:,}")

        logger.info(
            "é–‹å§‹è™•ç†æ‰¹æ¬¡",
            extra={
                "event": "batch_started",
                "job_run_id": self.tracker.job_run.id,
                "batch_num": chunk_num,
                "raw_count": original_count,
            },
        )
        # Transform: æ¸…ç†è³‡æ–™
        df_clean, errors = self.transformer.process(df_chunk, chunk_num)
        self.stdout.write(f"  æ¸…ç†: {original_count:,} â†’ {len(df_clean):,} ç­†")

        self.tracker.add_failed(len(errors))
        self.tracker.add_total(original_count)

        # çµ±è¨ˆé‡è¤‡ç­†æ•¸
        duplicates_count = sum(1 for e in errors if e["type"] == "DUPLICATE")
        self.tracker.add_duplicates(duplicates_count)

        # è¨˜éŒ„éŒ¯èª¤
        if errors:
            # If any row has error, record which batch it is in has error
            self.tracker.record_errors(errors, chunk_num)
            self.stdout.write(self.style.WARNING(f"  âš ï¸  é©—è­‰å¤±æ•—: {len(errors)} ç­†"))

            logger.warning(
                "æ‰¹æ¬¡é©—è­‰æœ‰éŒ¯èª¤",
                extra={
                    "event": "batch_validation_errors",
                    "job_run_id": self.tracker.job_run.id,
                    "batch_num": chunk_num,
                    "error_count": len(errors),
                },
            )
        # Load: è¼‰å…¥è³‡æ–™åº«
        if not df_clean.empty and not self.dry_run:
            success_count = self.loader.insert(df_clean)
            self.tracker.add_success(success_count)
            self.stdout.write(
                self.style.SUCCESS(f"  âœ… æˆåŠŸåŒ¯å…¥: {success_count:,} ç­†")
            )
            logger.info(
                "æ‰¹æ¬¡è™•ç†å®Œæˆ",
                extra={
                    "event": "batch_completed",
                    "job_run_id": self.tracker.job_run.id,
                    "batch_num": chunk_num,
                    "records_processed": success_count,
                },
            )
            # æ›´æ–°é€²åº¦
            self.tracker.update_progress(chunk_num)
        elif self.dry_run:
            self.stdout.write(
                self.style.NOTICE(f"  ğŸ” DRY RUN: å°‡åŒ¯å…¥ {len(df_clean):,} ç­†")
            )
            logger.info(
                "Dry run æ‰¹æ¬¡é è¦½",
                extra={
                    "event": "batch_dry_run",
                    "job_run_id": self.tracker.job_run.id,
                    "batch_num": chunk_num,
                    "would_process": len(df_clean),
                },
            )

    """
    ===== Load ====
    """

    def _print_summary(self):
        """è¼¸å‡ºåŸ·è¡Œæ‘˜è¦"""
        duration = self.tracker.job_run.duration_seconds or 0
        success_rate = self.tracker.job_run.success_rate
        stats = self.tracker.stats

        self.stdout.write(
            self.style.MIGRATE_HEADING(f"\n{'=' * 60}\nåŸ·è¡Œæ‘˜è¦\n{'=' * 60}\n")
        )

        self.stdout.write(f"åŸ·è¡Œ ID:      {self.tracker.job_run.id}")
        self.stdout.write(f"ç‹€æ…‹:         {self.tracker.job_run.get_status_display()}")
        self.stdout.write(f"åŸ·è¡Œæ™‚é–“:     {duration:.2f} ç§’")
        self.stdout.write("\nè™•ç†çµ±è¨ˆ:")
        self.stdout.write(f"  ç¸½ç­†æ•¸:     {stats['total']:,}")
        self.stdout.write(
            self.style.SUCCESS(
                f"  âœ… æˆåŠŸ:    {stats['success']:,} ({success_rate:.2f}%)"
            )
        )

        if stats["failed"] > 0:
            self.stdout.write(self.style.ERROR(f"  âŒ å¤±æ•—:    {stats['failed']:,}"))

        if stats["duplicates"] > 0:
            self.stdout.write(
                self.style.WARNING(f"  ğŸ”„ é‡è¤‡:    {stats['duplicates']:,}")
            )

        self.stdout.write(f"\n{'=' * 60}\n")

        # æç¤ºæŸ¥çœ‹è©³ç´°éŒ¯èª¤
        if stats["failed"] > 0:
            self.stdout.write(
                self.style.NOTICE(
                    f"\nğŸ’¡ æŸ¥çœ‹è©³ç´°éŒ¯èª¤:\n"
                    f"   python manage.py shell\n"
                    f"   >>> from tax_registration.models import DataImportError\n"
                    f"   >>> DataImportError.objects.filter(job_run_id={self.tracker.job_run.id})\n"
                )
            )
        # === çµæ§‹åŒ– log çµ¦ CloudWatch ===
        logger.info(
            "ETL åŸ·è¡Œæ‘˜è¦",
            extra={
                "event": "etl_summary",
                "job_run_id": self.tracker.job_run.id,
                "status": self.tracker.job_run.status,
                "duration_seconds": round(duration, 2),
                "success_rate": round(success_rate, 2),
                "records_total": stats["total"],
                "records_success": stats["success"],
                "records_failed": stats["failed"],
                "records_duplicates": stats["duplicates"],
            },
        )

    def _confirm_truncate(self) -> bool:
        """ç¢ºèªæ¸…ç©ºè³‡æ–™"""
        count = TaxRegistration.objects.count()
        self.stdout.write(
            self.style.WARNING(f"\nâš ï¸  åŸ·è¡Œå…¨é‡è¦†è“‹: å³å°‡åˆªé™¤ {count:,} ç­†ç‡Ÿæ¥­ç™»è¨˜è³‡æ–™!")
        )
        if self.auto:
            return True  # è‡ªå‹•åŒ–æ¨¡å¼ç›´æ¥é€šé

        answer = input("ç¢ºå®šè¦ç¹¼çºŒå—? (yes/no): ")
        return answer.lower() == "yes"

    def _truncate_tables(self):
        """æ¸…ç©ºè³‡æ–™è¡¨"""
        self.stdout.write("ğŸ—‘ï¸  æ¸…ç©ºè³‡æ–™è¡¨...")

        # Faster deletion
        with connection.cursor() as cursor:
            cursor.execute("TRUNCATE TABLE tax_registration CASCADE;")
            cursor.execute("TRUNCATE TABLE business_industry CASCADE;")

        self.stdout.write(self.style.SUCCESS("  âœ… å®Œæˆ"))

    def _should_continue_on_error(self) -> bool:
        """è©¢å•æ˜¯å¦ç¹¼çºŒ"""
        if self.dry_run or self.auto:
            return True

        answer = input("\nç™¼ç”ŸéŒ¯èª¤,æ˜¯å¦ç¹¼çºŒä¸‹ä¸€æ‰¹æ¬¡? (yes/no): ")
        return answer.lower() == "yes"
